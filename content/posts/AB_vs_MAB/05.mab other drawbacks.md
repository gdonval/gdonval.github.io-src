Title: MAB other drawbacks and conclusion
Date: 2016-05-19 14:58
Category: Data processing
Series: AB vs. MAB
Slug: MAB-other-drawbacks-conclusion
Authors: GaÃ«l

Until now, I have spent time (and CPU power) to precisely illustrate what the
two methods were designed for:

*   A/B testing efficiently answers the question *is there a difference?*
*   The MAB strategy provides the highest click-through rates over
    a wider range of sample sizes.

As their respective purposes are completely different, thinking of one of them as
intrinsically better than the other is nonsensical, especially as we have shown
that **each** method is clearly better than the other at what it was
designed for. Therefore saying that one is better than the other is saying that
a screwdriver is better than a hammer: it makes absolutely no sense because
each of them are useful for different things.

In this last post, I want to lay the emphasis on a few extra drawbacks of the
MAB strategy, mostly to show some side effects of the technique that were
seldom evoked in what I read.

The MAB strategy does skew the results
======================================

Results are skewed when there is a bias in the measurement process:
the "ideal" measured value is no longer equal to the hidden "true" value.

In statistics, there are a few dragons that are known to almost invariably
skew the results: changing the measurement process using the measurements
themselves is one of them.
The problem is that this kind of bias is hard for our brain to grasp
and, oddly enough, this is the very crux of most probability (false) "paradoxes"
(like the Monty-hall problem, 
[wiki link](https://en.wikipedia.org/wiki/Monty_Hall_problem)).

Back to the original blog post, the blogger claimed:

>   Showing the different options at different rates will skew the results. 
>   (No it won't. You always have an estimate of the click through rate for 
>   each choice)

That claim is rather strange: having data does not mean these data are not
skewed and of course showing different options at different rate won't
(usually) skew the results... however, as I said, changing these rates
**according** to the measurements most likely will...

So how can I prove that? I already gave an hint earlier in the series in that
figure:

![If you can't see the figure, please try another web browser which supports
SVG images]({filename}/images/difference_vs_sample_size.svg)

As I said at the time, there seems to be a remaining difference at higher
sample sizes but it is not obvious and really small. I can also plot the
distribution of these differences:

![If you can't see the figure, please try another web browser which supports
SVG images]({filename}/images/dist_difference_vs_sample_size.svg)

and the same conclusion applies: **if** there is a skew, it is not really 
visible: the two distributions are centered on $5\%$ and roughly
Gaussian-shaped (which allowed us to calculate and represent the arithmetic 
mean as the expected value earlier).

But remember, what *usually* introduces a skew is altering some probabilities
somewhere using the data themselves as an input. This situation is
likely to occur more often if the difference in click-through rates is small
or even null. I therefore calculated the same kind of distributions but in a
setup with equal click-through rates:

![If you can't see the figure, please try another web browser which supports
SVG images]({filename}/images/dist_zero_difference_vs_sample_size.svg)

The Gaussian-shaped distribution suddenly turned into a very strange one: 
it is still roughly symmetrical but the
expected value ($0\%$) is actually **not** the most probable value anymore.

As we can already guess, we should be able to see what happens more clearly
by calculating the difference between the click-though rate of the variant which
happened to be mostly favored with the one which happened to be mostly
disfavored. The resulting distribution are represented in the following figure:

![If you can't see the figure, please try another web browser which supports
SVG images]({filename}/images/abs_dist_zero_difference_vs_sample_size.svg)

We can observe that:

*   in the case of A/B testing, the distribution is a half of a Gaussian with
    its maximal value reached at $0\%$, as expected;
*   in the case of the MAB strategy, the distribution is not Gaussian anymore
    and the maximal value is reach at $0.4\%$.

The MAB strategy therefore skews the data themselves by introducing a
difference where there shouldn't be any.

This is an issue for two reasons:

1.  Actually the individual click-through rate distributions are skewed too,
    which basically means that we cannot trust any contingency test performed
    on the data gathered using the MAB strategy.
2.  There is no way to know whether a given difference is **actually** a difference.

These two extra reasons also justify why applying the MAB strategy to drug testing
and the likes is a very bad idea.

No test, no confidence: no conclusion?
======================================

Most people seem to forget completely about the test part of A/B **testing**.
The reasoning here, as seen elsewhere, is that whether or not the test is
positive, what appears to be the best-performing variant would be used anyway.
Therefore the test itself is useless; A/B testing itself is
useless; A/B testing can be replaced with something providing higher
click-through rates.

But the actual *test* in A/B testing is actually intended as a feedback, a way to
**estimate** your confidence in the results you obtained and make decisions
with both pieces of informations. In contrast, the MAB strategy seems to be
used far more often relying on one's "good luck": it is **assumed** that the
population is large enough to eventually provide meaningful results and the
problem is precisely that this is **usually** true.

Think of a successful campaign as a light bubble. When you switch it on, you
expect to get light and this is what **usually** happens. 
The MAB strategy is like
saying "let's ask a blind person to turn that light on: he will move more easily in
the dark, in that sense he will be more efficient for the job".
On the other hand, the A/B testing method would be more like saying 
"let's ask a sighted person
to switch it on because he needs to know whether he was successful and report
back". Yes, in most cases, the light will be on anyway!
However, there is no way to detect that the light did not turn on for whatever 
reason in the MAB strategy.

Mind you, the final and allegedly real results given in the original blog
post do not even pass the contingency test (the maximal $p$-values is only $0.7$).
So strictly speaking, his conclusions are questionable, especially given that
the MAB strategy is known to introduce such a difference.


Conclusion
==========


Other drawbacks
---------------

There are a few moot moot points that I did not evoke such as the effect of
time-varying click-through rates, the effect of the lack of equally-sampled 
control group, the implications of the actual overlap of the click-through rate
distributions depending on the method used, the relative spread of the
distributions, etc. I decided not to include them because the most disastrous
outcomes would require stringent (albeit not that rare) requirements. 
I did not want to weaken the whole series because some would dismiss these 
arguments saying "what are the odds?".

Original post
-------------

I've generally not spoken about the original blog itself throughout
the series. The main reason is that everyone has the right not to be completely
right and nagging on each moot point would not have been right.

That said, is seems important to indicate that if one does not want to talk
about actual A/B **testing**, but A/B testing **data-gathering** strategy, it would
have been less misleading to talk about *MAB with $\varepsilon = 0\%$*
vs. *MAB with $\varepsilon = 90\%$* as it is rigorously what is compared in
that post.

As well, A/B testing was not designed in a set-and-forget state of mind: there
are two different stages (*testing* and *exploitation*) that must be used. In
particular, conflating the two as done in the original post is misleading. 

Finally, sorry to raise that specific point but it illustrates well the degree
of understanding of the original blogger:

>   In the epsilon-first strategy, you can explore 100% of the time in 
>   the beginning and once you have a good sample, switch to pure-greedy.

is an **exact and accurate** description of what A/B testing is (assuming the
"good sample" part is assessed with a contingency test).

I won't go further down that road: the point is made and not knowing that tiny
bit is a shame for someone who blogged about it. But it is perfectly 
understandable and fine (it happens to everyone and I guess that series of
posts does contain its share of inaccuracies and incomplete understanding). 
In particular, this does not make him a bad developer, far from it.

Of hammers and screwdrivers
---------------------------

As surprising as it may appear, A/B testing and the multi-armed bandit strategy
were designed for two completely different purposes:

*   The purpose of A/B testing is to determine whether or not there is a
    difference and provide that answer (with the uncertainties) through a
    statistical test.
*   The purpose of the multi-armed bandit strategy is to maximize the reward
    over a large range of sample sizes. Usually it can (and rigorously should)
    be used in a set-and-forget mode.

These methods are just like a hammer and a screwdriver: both can be used to do
nearly the same thing, yet cannot really be freely swapped.


When should I choose A/B testing?
---------------------------------

*   When you need to really determine which variant is better (e.g. drug trials).
*   When you need to estimate the difference precisely (e.g. multivariate
    analysis).
*   When you need the shortest testing period possible for a given confidence
    in the results.
*   When you need to know the uncertainties on the results.
*   When you need to assess that there is actually no difference.

When should I use the MAB strategy?
-----------------------------------

*   When being sometimes wrong without any indication of it is not that
    important.
*   When you cannot setup an optimal A/B testing campaign because you lack too
    much information (minimum population size, estimate of the difference,
    etc.).

The End?
--------

What this post series lacks (as most post out there) are references: I am
pretty sure that all this work and far far more has already been done by people
out there. Though I cannot personally recommend it (as I have not read it), I
know there is a short [O'Reilly book](http://shop.oreilly.com/product/0636920027393.do?sortby=publicationDate) 
on the subject of bandit algorithms in general. I also know that there are a
lot of references in Google scholar about both methods.

Literature scanning is not just for scientist: it is necessary to efficiently
reuse the knowledge humankind already gathered on the subject in order to 
eventually avoid the traps, pitfalls and inefficiencies caused by starting from
scratch. I am sure a lot of great minds already worked on this, this is what
you should read, not some random blog posts on the internet if you really want
to do things correctly.

As far as this blog is concerned, it just started out as a few tests and it is
mainly intended to show once again that the world is not just black and white
by testing some of the original blogger claims.
