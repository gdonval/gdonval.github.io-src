:title: The pros of the A/B testing method
:date: 2016-05-15 12:26
:status: published 
:category: Data processing 
:series: AB vs. MAB
:series_index: 3
:slug: pros-of-AB-testing 
:authors: GaÃ«l


.. image:: {filename}/images/dab.gif
    :align: center


Test setup
==========

As we have already said, when a (statistical) test is applied to data, 
it answers the question for which it was designed with a given
*level of significance* and *power*; two components of what we could 
improperly call "level of confidence" for simplicity. Here:

-  The *level of significance* is the probability of correctly
   concluding that the variants (|orangeBtn| and |greenBtn|
   here) *have no significant impact* on the click-through rate.

-  The *power* is the probability of correctly concluding that the
   variants *have a significant impact* on the click-through rate.

These two values in turn depend on the chosen sample size and the actual
difference in click-through rate: greater sample sizes and click-through rate
differences would both increase the level of significance and the power of the
test.

Given a minimum level of significance, a minimum power and a detection
threshold (i.e. minimal significant difference to be identified), the sample
size is fully determined *from the start* in A/B testing.

In the multi-armed bandit framework, the sample size cannot be determined that
way basically because we don't know when and how many times the algorithm is
going to favour another variant (i.e. switch). 
In practice, this means that you have actually
no idea of the level of confidence to expect in that case.

*Really? No idea at all?!* Actually, you could resort to mathematics and even
numerical modelling to get a proper estimate. However that estimate depends on
the current state of the algorithm (so it is calculated after, not before).
That said, I will show some hints that the proposed multi-armed bandit
strategy do introduce a bias that would invalidate our confidence estimates.
Anyway, there seems to be no straightforward way to properly setup the
multi-armed bandit strategy from the start. 

On the other hand, the multi-armed
bandit strategy can be setup in a *set-and-forget* mode. You would not get any
more information about the confidence in the results, but you could use your
whole population instead of a sample of it and yet keep a high overall
click-through rate.

A/B testing is more powerful
============================

Now, determining that the variants do have an impact on the
click-through rate seems more important that proving that there is none.
So, it seems interesting to represent the power of each method at a given
level of significance to see how large the sample should be to reach a given
power:

.. figure:: {filename}/images/power_vs_sample_size.svg?

    Monte-Carlo estimation of the power of Fisher's exact contingency test
    as a function of the sample size (i.e. the number of visitors "spent" in
    each method) at a given level of significance.

    significance: level of significance of the contingency test.

    :math:`P(O|A)`: (hidden) real click-through rate of the :math:`A` 
    (|orangeBtn|) website variant used as an input in the simulation.

    :math:`P(O|B)`: (hidden) real click-through rate of the :math:`B` 
    (|greenBtn|) website variant used as an input in the simulation.

    error bands: error bar on the averaged power value.

It appears clearly that A/B testing reaches higher powers
at far smaller sample sizes: A/B testing typically requires sample sizes *5
times* smaller than the multi-armed bandit strategy!

*Why is that?* As I claimed before when I presented the A/B testing
method, it is generally more efficient to provide each variant as often
than it is to favour one. We will see by the end of this series that A/B
testing seems always more powerful than the multi-armed bandit strategy with
non-zero :math:`varepsilon`.

A/B testing is correct more often at small sample sizes
=======================================================

The problem with power estimations is that they are tied to one test.

From what I have seen the original blog author and quite a few people out there
really don't see the appeal of proper testing (e.g. the original blog author 
seems to believe that A/B testing is one-staged and he never ever talks about
confidence, significance, power and even testing). In a nutshell: they are
resorting to their good luck alone to provide a correct answer.

From the testing perspective however, **assuming** that there is a difference
anyway is equivalent to choosing a level of significance of :math:`0\%`. 
Yet that does not mean that the power would reach :math:`100\%`: we can still 
choose the wrong variant. 

So, if we ditch the test all together, would the multi-armed
bandit method be correct more often than A/B testing? The answer turned out to
be (at least generally) *no*:

.. figure:: {filename}/images/correct_vs_sample_size.svg?

    The A/B testing data-gathering strategy and the multi-armed bandit strategy
    were used to estimate the click-through rates of both variants of the
    website. 

    :math:`P(O|A)`: (hidden) real click-through rate of the :math:`A` 
    (|orangeBtn|) website variant used as an input in the simulation.

    :math:`P(O|B)`: (hidden) real click-through rate of the :math:`B` 
    (|greenBtn|) website variant used as an input in the simulation.

    error bands: error bar on the averaged power value.

In this example, the A/B framework again provide better results at
smaller sample sizes. For instance, the A/B framework only requires
:math:`300` trials to be correct :math:`9` times out of
:math:`10`. To provide the *same* guarantee, the MAB strategy would
require :math:`600` trials: twice as many!

This can be explained the same way as above: providing each website
variant as often to the visitors to get the best-performing variant is
more efficient. This also means that comparing the performances of the two 
methods at the same sample size is largely misleading.

A/B testing provides more accurate estimations of the differences
=================================================================

A correct estimation of the differences in click-through rates is
important: it is obviously required in methods relying on its quantifications
(e.g. multivariate analysis) but also to actually determine which
variant performs better as stating that

.. math:: a > b

is equivalent to

.. math:: a - b > 0\text{.}

I estimated the difference in click-through rates in the same setup
as above and obtained the following figure:

.. figure:: {filename}/images/difference_vs_sample_size.svg?

    Estimates of the difference in click-through rate at different sample sizes
    using the A/B testing data-gathering strategy and the multi-armed bandit
    strategy (:math:`\varepsilon = 90\%`). The A/B testing method converges
    needs a smaller sample size to reach the expected value of
    :math:`P(O|B) - P(O|A) = 5\%`.

    :math:`P(O|A)`: (hidden) real click-through rate of the :math:`A` 
    (|orangeBtn|) website variant used as an input in the simulation.

    :math:`P(O|B)`: (hidden) real click-through rate of the :math:`B` 
    (|greenBtn|) website variant used as an input in the simulation.

    error bands: error bar on the averaged power value.

We can clearly see that the A/B testing data gathering method converges
toward the expected value of :math:`5\%` much faster than the MAB
strategy. Worse, that remaining difference, though small, actually goes
on and on for a very long time. This remaining difference is not enough to
talk about a bias in the results yet, this is one of the many reasons why
we need to go further there.

Summary: what is A/B testing good at?
=====================================

These results illustrated what it means for A/B testing to answer the
question: *Is there a difference?*

-  Its data-gathering scheme is **efficient**: it provide a very high power
   in the contingency tests, it leads to correct results more often at
   lower sample rates than with the MAB strategy.
-  It makes use of the contingency test to actually answer that question
   at a given "level of confidence" (significance and power). It is also
   used to determine what the sample size should be from the start.
-  It also generally provides more precise and more accurate estimates
   of the actual difference in click-through rates.

Beyond demonstrating that the presented multi-armed bandit strategy **is not
systematically better** than A/B testing, the results outline another important
fact: this multi-armed bandit strategy requires a larger sample size to provide
the same practical guarantees as A/B testing.

From my perspective, this is just like an insurance seller undermining
the competition by comparing the price of his most basic contract with
the price of a broader contract (covering more stuff). Yes the basic
contract is cheaper but this is comparing apples to oranges.

That said, that basic contract might very well be sufficient for web optimisation
and this is what I am going to assess in the next post.

.. |greenBtn| image:: {filename}/images/green_btn.svg 
    :align: middle
.. |orangeBtn| image:: {filename}/images/orange_btn.svg
    :align: middle


