Title: Reward maximisation
Date: 2016-05-18 16:17
Category: Data processing
Series: AB vs. MAB
Slug: reward-maximisation
Authors: Gaël


I showed in the previous section that the A/B testing data-gathering method
by itself has some advantages over the MAB strategy.

But how does this translates to *what really matters*™, clicks?

In contrast with the previous section, we are not just interested in the
*testing stage* anymore: we want to know which method would provide the highest
click-through rate overall (so during testing and after, once the
alleged best-performing variant is put in production). 
I will first give an example in a specific setup
and then explore the difference in behavior in a much broader scope to avoid
making "general" conclusions out of a single specific example.

Effect of the sample size
=========================

So, I first simulated a large number of complete campaigns 
(i.e. testing stage followed by an exploitation stage using the testing results).
I assumed here that the population is composed of $20000$ visitors in total.
In this particular example, I used the two-stepped variant of the MAB strategy
provided that the set-and-forget mode is reached when the sample size is equal
to the population size. I obtained the following figure:

![If you can't see the figure, please try another web browser which supports
SVG images]({filename}/images/clicks_vs_sample_size.svg)

As we can see, there are different areas where each method shines but we are
going to get back to that in detail a bit later. 

The asymptotic behavior of the two two-stepped methods are however similar:

*   We can see that the total click-through rate rises quickly as smaller 
    sample sizes (left of the figure), yet A/B testing systematically reaches 
    higher click rates at smaller sample sizes than in the two-stepped MAB 
    strategy.
*   A maximum is then reached in both methods: that maximum is reached, again,
    at smaller sample sizes in the A/B testing framework.
*   Finally, the total click rate decreases linearly toward the final value in
    each case (which corresponds to the set-and-forget scenario). These limit
    values are known and could be calculated in each case: for A/B testing
    this is generally the average of the individual click-through rates whereas
    in the MAB strategy, this average is weighted so that higher values are
    obtained.

This kind of curves seems to be generally misunderstood: as the MAB strategy
provides higher values on the largest range, people tend to think that the MAB
strategy is therefore generally better. **This is incorrect**.

What this curve really means is that A/B testing does actually compete fairly
well with the MAB strategy in terms of click-through rates. But let me explain
using the following close-up:

![If you can't see the figure, please try another web browser which supports
SVG images]({filename}/images/closer_clicks_vs_sample_size.svg)

Roughly the same total number of clicks can be obtained in A/B testing, 
using smaller sample sizes (and therefore shorter testing periods).
So there are conditions in which A/B testing performs as good as the MAB 
strategy click-wise! All we have to do is to find these conditions. Easier
said than done, but it should be possible using the same tools that allows us to
determine the sample size until now.

That said, the MAB strategy behaves better than A/B testing asymptotically:
this is actually what this algorithm is good at and what it was designed for.
That property could come in handy: it could be considered as a
safety net in many situations and it makes the set-and-forget mode something
that can be actually used.

However, telling when to stop in the two-stepped case is even harder than with
A/B testing (because we don't know how many trials the algorithm is going to
spend for each variant, especially for smaller sample sizes).

The alleged click-through rate superiority of the MAB strategy is not that
clear anymore, in fact A/B testing seems to provide equivalent results at even
smaller sample sizes. However, this does not mean that the MAB strategy
performs badly click-wise. Moreover, these results were obtained in a single
setup. It is now time to explore a bit more.


Population size
===============

In the above example, the sample size was chosen equal to $20000$. But what
would happen to the total click-through rate if that population size was
different? This is what I assessed in the following figure:

![If you can't see the figure, please try another web browser which supports
SVG images]({filename}/images/diff_vs_sample_size_vs_tot.svg)

The blue patches represent conditions in which A/B testing provide
significantly better click-through rate on average than the MAB strategy.
The opposite situation is represented by the green patches. The white patches
represent conditions in which both methods give roughly the same click-through
rates.

It appears clearly that A/B testing is systematically better at lower sample
sizes. As the population increases, the range of sample sizes in which
A/B testing performs better becomes wider. Though not represented on the
figure, the difference in the maximum click-through rate between the methods
is negligible. This means that A/B testing can systematically reach the same
number of clicks as the MAB strategy.
Again, the MAB strategy performs obviously as well at higher sample sizes.

This can be explained by the asymptotic behaviour of the two methods as
illustrated previously: as the population size increases, the worse-case
scenario (i.e. the set-and-forget mode where all the time is spent in the
testing stage) is reached further and further on the right
resulting in a much gentler slope in both methods. However, as the slope in the
A/B testing method was much steeper to begin with, the improvement is more
spectacular there, resulting in a wider range of superiority.

MAB asymmetry
=============

As I have explained earlier, the core of the MAB strategy consists in favouring
the best-performing variant. The strength of this favour is quantified by a
variable called $\varepsilon$ (the Greek letter "epsilon"). 
When $\varepsilon = 0\%$, the
MAB strategy favours the seemingly best-performing variant by $0\%$: this is
equivalent to proper A/B testing data-gathering (no variant is ever favored). 
When $\varepsilon = 100\%$, the algorithm is trapped: whichever variant was 
found the best initially will be the only one served.

![If you can't see the figure, please try another web browser which supports
SVG images]({filename}/images/epsilon_vs_sample_size_vs_tot.svg)

Again, A/B testing seems to perform better at lower sample sizes. Though not
shown in the figure, the maximal value reached with each method for each
$\varepsilon$ is again nearly equal (i.e. click-wise, the two methods perform 
as well, again).

It also appears clearly that, in this particular setup,
the two methods are roughly equivalent below $\varepsilon = 50\%$
at smaller sample sizes. Yet the MAB strategy remains better at higher sample
sizes. 
This means that, click-wise, the MAB strategy in these setups is always as good
or better than A/B testing on average. The price to pay is obviously that the
MAB strategy gives worse and worse results at even higher sample sizes
(especially you don't want a small $\varepsilon$ in the set-and-forget mode).

This can be explained by considering yet again the first figure of this post:
$\varepsilon$ basically determine the right-hand asymptotic value: 
the higher $\varepsilon$, the higher the value provided the population size is
large enough. "Large enough" depends on $\varepsilon$ too: the higher
$\varepsilon$, the larger the population should be. This is a trade-off and an
optimum $\varepsilon$ can be computed given the population size using
simulations.

Click-through rate reference value
==================================

Until now, I have basically always set one click-through rate at $10\%$ 
and the other at $15\%$ (corresponding to a difference of $15 - 10 = 5\%$)
and I want to see what would happen if I introduced an **offset** to shift these
values upward. This is what I obtained:

![If you can't see the figure, please try another web browser which supports
SVG images]({filename}/images/POXorig_vs_sample_size_vs_tot.svg)

It appears very clearly that the largest range of A/B testing superiority is
obtained for an offset of $\sim 35\%$: this corresponds to the actual
click-through rates $45\%$ and $60\%$, i.e. almost centered on $50\%$.

This result is actually cause by a change in the relative power of the two
data-gathering strategies: when the click-through rates are around $50\%$, the
A/B testing method finds the correct variant even faster than in other cases
while the MAB strategy is not really better because it keeps favouring one
variant at the expense of the others.

Difference in click-through rates
=================================

The last parameter I wanted to study is the difference itself between the
click-through rates. This is what I represented in the following figure
(the ${\Delta}P(O|X)$ notation represents this difference):

![If you can't see the figure, please try another web browser which supports
SVG images]({filename}/images/POXdiff_vs_sample_size_vs_tot.svg)

It appears clearly that the MAB strategy provides a far broader range of
superiority than A/B testing. Should we conclude that the MAB strategy is
simply better for large differences? Not this time: there is always a range of
values in which A/B testing provides better results but it is smaller: we would
need a far finer grid to see it. For instance, a difference of $50\%$ can be
correctly identified $95\%$ of the time with only a few dozens of trials while
the minimum sample size represented here is actually $100$.

The wider range of click-wise superiority of the MAB strategy here can be
explained again by the power of the strategies: higher differences are much
easier to identify (it requires far fewer trials to do so). Therefore the MAB
strategy gets correct for smaller sample sizes and gets the advantage. Yet,
there is no inversion in the relative power of the methods: A/B testing remains
better but the MAB strategy catches up faster.

Summary: what is the MAB strategy good at?
==========================================

In practice, saying that this particular implementation of the multi-armed
bandit strategy has been developed to maximise the number of clicks appears 
misleading. This is **not** what it does.
What it was actually designed for (and does very well) is keeping that total
number of clicks very high for larger samples sizes. See the difference?

It is even possible to use the MAB strategy in a set-and-forget mode and yet
get acceptable results: no initial calculations, basically nothing to do,
a real no-brainer. However, this comes at a price: that mode yields worse
results than both two-stepped strategies. Again, this is a trade-off.

*Is that all? Then why would the original blogger present the MAB strategy as
something that always beats A/B testing?* Simple: a fair amount of zealotry
coupled with overconfidence and a shallow knowledge. *Really?!* Well, most
likely yes, but that's not all: there is not just one single way to design an A/B
campaign. 

Depending on the context, the optimal sample size calculated to be used in A/B
testing can vary greatly and fall off pretty far away from the optimal settings
from the maximum reward point of view. The reason is that gathering the maximal
number of clicks on average (i.e. over multiple campaigns) is not the same as
providing a correct answer with a particular "level of confidence".

Just one example: assume that you defined the sample size to that
you have a $95\%$ chance to correctly identify a $1\%$ difference. As it
turned out, there was actually a difference of $5\%$. 

From the test
perspective, this is good: you are even more confident that the variant you
found os the best is indeed the best.
Click-wise, this is another story: you spent more time testing at the expense
of the total number of clicks. This is the huge drawback of A/B
testing: it has not been designed to yield a high click-through rate in a wide
range of sample sizes so even a few hundred extra trials can lead to a dramatic
decrease of the total number of clicks.

In that specific way, the MAB strategy do behave better in general provided
that the population size is large enough. This is just a different trade-off.

*So, in practice, does that mean that using the MAB strategy is all right?*
Most likely, yes. Though there are other drawbacks to consider: the next (and
last post) of this series is dedicated to the description of a few of them.
