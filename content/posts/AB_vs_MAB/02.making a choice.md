Title: Choosing the best variant
Date: 2016-05-14 14:41
Category: Data processing
Series: AB vs. MAB
Slug: making-a-choice
Authors: GaÃ«l



Have you ever been given a choice that you found very hard to make?
In some situations, that choice is hard because you don't like any of the
outcomes.
In other situations, it is hard because you just **don't know** what these
outcomes would be. In that latter case, you would generally try to
predict<todo:link to modelling>
what these outcomes would be either using pure intuition, common sense,
your knowledge and experience, or even fancy mathematical stuff.

In the web-service realm, there is one such a question that haunts a lot of
people, day after day, and even at night in their worst nightmares:

> Is there anything we can do to make more money out of my website?

Website optimization
====================

*What is he talking about?* could you ingenuously say... People, like a lot
of other things, tend to react the same way *on average* given the same
stimulus: people tend to take an umbrella or a rain coat with them when it
is raining. Nothing really surprising there.

But did you know that a simple [cup color could change the way you taste
chocolate](http://dx.doi.org/10.1111/j.1745-459X.2012.00397.x)?
In a very similar way, changing a simple button color on a website for instance
could increase the total number of clicks, yielding more incomes. The process
of finding such an
optimal design is called *website optimization*.

Of time machines and statistics
================================

But how do we know that one variant yields a higher reward? Strictly speaking,
knowing it *exactly* would require a time machine to test all the variants in
the *exact* same conditions. In one timeline, you could provide the first
variant. In the other timeline you could provide the alternative variant and
then use your time machine a last time to settle on the best-performing
variant.

However, if we can afford to be wrong *sometimes*,
there is another way to answer that question without breaking any
physical law: *statistics*<todo:link to statistics>.

In our button color example we want to get the maximum number
of clicks. This is our objective as well as our reward.
That number of click is not easy to handle: obviously if 5 people out of 10
clicked, it is not the same as 5 people out of 10000. That is why we are really
looking at is the click-through rate: the number of people who clicked over the
number of people who visited the website.

However, if you spend your time (and your visitors) assessing which variant is
performing the best, your reward (i.e. total number of clicks) might be rather
disappointing. This is why the testing campaign is usually performed on a
subset of the total visitor **population**. That subset is called a **sample**
of the population. That sample will be subjected to each variant of the
website, in our example:

* one variant of the website, would contain an orange
![*Buy Now!*]({filename}/images/orange_btn.svg) button
* while the other variant would contain a green 
![]({filename}/images/green_btn.svg) one.

Once the testing period is over, the seemingly best-performing variant is
served to all the remaining visitors (this is sometimes called the exploitation
period).

"Problem solved!" you said? Unfortunately, nope. At this stage, we have only
reached a precision which is akin to "people die from hunger, so let's give
them food": if this was enough, people wouldn't die from hunger anymore.
The devil**s** are in the details: how large should that sample be?
how often are we expecting to be wrong? how should be provide the variants
exactly? *etc.*

This is exactly why some *specific* methods were devised,
such as A/B testing ([wiki](https://en.wikipedia.org/wiki/A/B_testing))
and the 
multi-armed bandit strategy 
([wiki](https://en.wikipedia.org/wiki/Multi-armed_bandit)). And these are the
one I am going to study.

If that part is not clear enough, here is a longer description.

A/B testing
===========

The purpose for which A/B testing was designed is to answer a single question:

> Is any of my variants (i.e. ![the orange]({filename}/images/orange_btn.svg)
> and ![the green button in our example]({filename}/images/green_btn.svg)) 
> inducing a significantly different outcome (i.e. click-through rate here)?

As explained above, A/B testing<todo: link to details> is a two-staged method:

1.  *Testing stage*: this is where the data-gathering and analysis take place.
2.  *Exploitation stage*: this is where only the best-performing variant is served.

Proper A/B testing requires all the variants to be served almost as often at
any point of time. Even though this is in no way a hard requirement, we will
see that is is actually a very efficient way to get the highest confidence in
our data.

So where is the "testing" of *A/B testing* coming from? The actual testing is
performed at the end of the *testing stage* to provide an actual answer to the
question above. That test in called a *contingency test* and for this kind of
*Bernoulli processes* (i.e. binary data like "has clicked or not?") we are
going to use *Fisher's exact test*
([wiki](https://en.wikipedia.org/wiki/Fisher's_exact_test)).

Why is such a test important? Because it answers the question above while
providing an estimate of what we could call *confidence*. With A/B testing we
can say:

> We proved that the green button yields higher click-through rates with only
> a $1\%$ chance of being wrong here.

This is a very precious piece of information if the existence of a difference
matters (like in drug trials to name one).


The multi-armed bandit method
=============================

The multi-armed bandit method was allegedly designed for only one purpose:

> Maximizing the reward (i.e. the total number of clicks in our case study).

The particular multi-armed bandit strategy presented
in the original blog post was not specifically designed in a two-staged 
fashion (even though it *could* be used that way). 
It was instead presented in a *set-and-forget* mode, in
which there is no testing stage: the method is applied on the total population.

It works by first determining which variant worked the best so far. That
variant is then favoured and is served more often than the alternative.
The instant another variant is found better (as more data are gathered),
it is served more often.

In contrast with A/B testing, there seem to be no fool-proof way to estimate
a suitable sample size without making a few assumptions. In that sense, the
set-and-forget mode is more a workaround than a feature of the method.

There is also no test associated with that strategy. A contingency test could
be applied but we will see that there are drawbacks to that.

To sum it up, the multi-armed bandit method is more accurately described as a
strategy: it does not answer any question. This is generally not a problem
because the sample sizes people use are usually large enough to provide **a**
correct answer.

Main differences between the methods
====================================

In practice, both methods could be used in the *two-stepped* mode and a
contingency test can be performed in both.

However, in contrast with a few claims of the original blog
post:

*   A/B testing is only a specific name given to a 2-variant **bucket tests** 
    ($A$ and $B$, you guessed it). However nothing prevents us
    from using it on more variants (Fisher's exact test can be used on any
    number of variants or all the variants can be compared 2-by-2).
*   Variants can be added or removed any time. In particular, A/B
    testing does not *require* the variants to be served as often to work.
    However, serving them as often at any given time is actually more efficient
    and prevents a whole class of biases (I will come to that later).

So what is the real core difference?

> A/B testing serves each variant as often whereas the MAB strategy favours one.

That's it.


