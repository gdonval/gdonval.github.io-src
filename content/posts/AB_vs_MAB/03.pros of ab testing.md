Title: The pros of the A/B testing method
Date: 2016-05-15 12:26
Status: published
Category: Data processing
Series: AB vs. MAB
Slug: pros-of-AB-testing
Authors: Gaël

<center>![]({filename}/images/dab.gif)</center>

In order to bring some nuances and perspective into the discussion
(a.k.a. *unsupported claims smashing*™), this post is
dedicated to showing what A/B testing is good at and how it compares to the
multi-armed bandit strategy.

A/B testing is more powerful than MAB testing
=============================================

When a (statistical) test is applied to data, it answers the question for which
it was designed with a given *level of significance* and *power* (the two
components of what we could improperly call "level of confidence" for
simplicity):

*   The *level of significance* is the probability of correctly concluding 
    that the variants (![the orange]({filename}/images/orange_btn.svg)
    and ![the green button]({filename}/images/green_btn.svg) here) *have **no**
    significant impact* on the click-through rate.

*   The *power* is the probability of correctly concluding that the variants
    **have** *a significant impact* on the click-through rate.

The power is the most important estimator of confidence in our case:
if we see a difference, we would like to know if it is significant.
So we modelled<todo: link> the whole process and estimated the power in each
method at a given level of significance and for different sample sizes.

![If you can't see the figure, please try another web browser which supports
SVG images]({filename}/images/power_vs_sample_size.svg)

It appears clearly that A/B testing reaches higher (and better) powers at far
smaller sample sizes: it typically requires sample sizes *5 times* smaller
than with MAB!

*Why is that?* As we claimed above when presenting the A/B testing method, it
is generally more efficient to provide each variant as often than to favour
one. A/B testing is better here and we didn't even have to look for a
convoluted setup: this is valid for all the setups we tried!

A/B testing is correct more often at small sample sizes
=======================================================

There are cases however where no test is used. This is equivalent to saying
that you choose a level of significance of $0\%$. Yet that does not mean that
the power would reach $100\%$ (in this case, the power is equivalent to how
often the seemingly best-performing variant is really the best performer).

As it turned out, even in
that case, the A/B data-gathering strategy does provide better results:

![If you can't see the figure, please try another web browser which supports
SVG images]({filename}/images/correct_vs_sample_size.svg)

In this example, the A/B framework again provide better results at smaller
sample sizes. For instance, the A/B framework only requires $300$ trials
to be correct in $9$ campaigns out of $10$. To provide the *same* guarantee,
the MAB strategy would require $600$ trials: twice as many!

This can be explained the same way as above: providing each website variant as
often to the visitors to get the best-performing variant is more efficient.


A/B testing provides more accurate estimations of the differences
=================================================================

A correct estimation of the differences in click-through rates is important:
it is required in methods relying on its quantifications (e.g. multivariate
analysis) but also to actually determine which variant performs better:
$$a > b$$ is equivalent to $$a - b > 0\text{.}$$

I estimated the difference in click-through rates (denoted $P(O|B) - P(O|A)$ 
in the figure — see why here<todo: link>) in the same setup as above and
obtained the following figure:

![If you can't see the figure, please try another web browser which supports
SVG images]({filename}/images/difference_vs_sample_size.svg)

We can clearly see that the A/B testing data gathering method converges toward
the expected value of $5\%$ much faster than the MAB strategy. Worse, that
remaining difference, though small, actually goes on and on for a very long 
time.

Summary: what is A/B testing good at?
=====================================

These results illustrated what it means for A/B testing to answer the question:
*Is there a difference?*

*   Its data-gathering scheme is efficient: it provide a very high power in the
    contingency tests, it leads to correct results more often at lower sample
    rates than with the MAB strategy.
*   It makes use of the contingency test to actually answer that question at a
    given "level of confidence" (significance and power). It is also used to
    determine what the sample size should be.
*   It also generally provides more precise and more accurate estimates of the 
    actual difference in click-through rates.

Beyond invalidating the most outrageous claims of the original blog post,
these results outline another very important fact: the MAB strategy generally
requires more samples to provide the exact same guarantees as A/B testing.
This means in particular that it would not be fair to compare click-through
rates at a single given sample size: that sample size should be adjusted separately
for each method in order to provide the same guarantees.

From my perspective, this is just like an insurance seller undermining the
competition by comparing the price of his most basic contract with the price 
of a broader contract (covering more stuff). 
Yes the basic contract is cheaper but this is comparing apples to
oranges. That said, that basic contract might very well be sufficient and this
is what I am going to assess in the next post.
